{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallelamanojkumar90/AIML/blob/main/Week5_Day5_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfFnmF8DnaDR"
      },
      "source": [
        "# Week 5, Day 5: Transformers and Attention Mechanisms\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand attention mechanisms\n",
        "- Learn transformer architecture\n",
        "- Master self-attention concepts\n",
        "- Practice implementing transformers\n",
        "\n",
        "## Topics Covered\n",
        "1. Attention Mechanisms\n",
        "2. Transformer Architecture\n",
        "3. Self-Attention\n",
        "4. Practical Applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBF3PvW4naDU"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.models import Model"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-hu1BFnaDX"
      },
      "source": [
        "## 1. Attention Mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpYuqnxWnaDX"
      },
      "source": [
        "def visualize_attention():\n",
        "    # Simple attention example\n",
        "    def attention(query, key, value):\n",
        "        # Compute attention scores\n",
        "        scores = np.dot(query, key.T)\n",
        "        # Apply softmax\n",
        "        weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "        # Compute weighted sum\n",
        "        output = np.dot(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    # Generate sample data\n",
        "    np.random.seed(42)\n",
        "    query = np.random.randn(1, 4)\n",
        "    key = np.random.randn(3, 4)\n",
        "    value = np.random.randn(3, 4)\n",
        "\n",
        "    # Compute attention\n",
        "    output, weights = attention(query, key, value)\n",
        "\n",
        "    # Visualize attention weights\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(weights, cmap='viridis')\n",
        "    plt.colorbar(label='Attention Weight')\n",
        "    plt.title('Attention Weights')\n",
        "    plt.xlabel('Key/Value Position')\n",
        "    plt.ylabel('Query Position')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Attention Weights:\")\n",
        "    print(weights)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "visualize_attention()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuE0wEf7naDY"
      },
      "source": [
        "## 2. Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdyBCOPTnaDZ"
      },
      "source": [
        "def self_attention_example():\n",
        "    # Create sample sequence\n",
        "    sequence = np.random.randn(5, 8)  # 5 tokens, 8 features\n",
        "\n",
        "    # Create multi-head attention layer\n",
        "    attention = MultiHeadAttention(num_heads=2, key_dim=4)\n",
        "\n",
        "    # Apply self-attention\n",
        "    output = attention(sequence, sequence)\n",
        "\n",
        "    # Visualize input and output\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(sequence, cmap='viridis')\n",
        "    plt.title('Input Sequence')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(output, cmap='viridis')\n",
        "    plt.title('Self-Attention Output')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "self_attention_example()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-lQU4_naDZ"
      },
      "source": [
        "## 3. Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp45gTkDnaDb"
      },
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Example usage\n",
        "def transformer_example():\n",
        "    # Create sample input\n",
        "    sequence = np.random.randn(1, 10, 32)  # batch_size=1, seq_len=10, embed_dim=32\n",
        "\n",
        "    # Create transformer block\n",
        "    transformer = TransformerBlock(embed_dim=32, num_heads=2, ff_dim=64)\n",
        "\n",
        "    # Apply transformer\n",
        "    output = transformer(sequence, training=False)\n",
        "\n",
        "    print(\"Input shape:\", sequence.shape)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "\n",
        "    # Visualize transformation\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(sequence[0], cmap='viridis')\n",
        "    plt.title('Input Features')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(output[0], cmap='viridis')\n",
        "    plt.title('Transformed Features')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "transformer_example()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU3_-iM-naDc"
      },
      "source": [
        "## Practical Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwTKVMw5naDd"
      },
      "source": [
        "# Exercise 1: Text Classification with Transformers\n",
        "\n",
        "def text_classification_exercise():\n",
        "    # Sample sentences\n",
        "    sentences = [\n",
        "        \"I love this movie!\",\n",
        "        \"This was a terrible experience.\",\n",
        "        \"The food was amazing.\",\n",
        "        \"I would not recommend this.\",\n",
        "        \"Great service and atmosphere.\"\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0, 1]  # 1: positive, 0: negative\n",
        "\n",
        "    print(\"Task: Build a transformer for sentiment analysis\")\n",
        "    print(\"1. Tokenize and encode text\")\n",
        "    print(\"2. Create transformer model\")\n",
        "    print(\"3. Train the model\")\n",
        "    print(\"4. Evaluate performance\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "text_classification_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBZYVMMGnaDe"
      },
      "source": [
        "# Exercise 2: Time Series with Attention\n",
        "\n",
        "def time_series_attention_exercise():\n",
        "    # Generate synthetic time series\n",
        "    np.random.seed(42)\n",
        "    t = np.linspace(0, 100, 1000)\n",
        "    series = np.sin(0.1 * t) + np.sin(0.01 * t) + np.random.normal(0, 0.1, len(t))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(series)\n",
        "    plt.title('Time Series Data')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Task: Implement attention for time series\")\n",
        "    print(\"1. Prepare sequence data\")\n",
        "    print(\"2. Create attention model\")\n",
        "    print(\"3. Train and evaluate\")\n",
        "    print(\"4. Visualize attention weights\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "time_series_attention_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ7C0gIcnaDf"
      },
      "source": [
        "## MCQ Quiz\n",
        "\n",
        "1. What is the main innovation of transformers?\n",
        "   - a) RNN architecture\n",
        "   - b) Self-attention\n",
        "   - c) CNN architecture\n",
        "   - d) Dense layers\n",
        "\n",
        "2. What is the purpose of multi-head attention?\n",
        "   - a) Faster training\n",
        "   - b) Multiple representation subspaces\n",
        "   - c) Reduced memory usage\n",
        "   - d) Simpler architecture\n",
        "\n",
        "3. What is positional encoding used for?\n",
        "   - a) Data normalization\n",
        "   - b) Sequence order information\n",
        "   - c) Feature extraction\n",
        "   - d) Regularization\n",
        "\n",
        "4. What is the complexity of self-attention?\n",
        "   - a) O(n)\n",
        "   - b) O(nÂ²)\n",
        "   - c) O(n log n)\n",
        "   - d) O(1)\n",
        "\n",
        "5. What is layer normalization for?\n",
        "   - a) Feature scaling\n",
        "   - b) Training stability\n",
        "   - c) Sequence ordering\n",
        "   - d) Memory efficiency\n",
        "\n",
        "6. What is the feed-forward network in transformers?\n",
        "   - a) RNN layer\n",
        "   - b) Dense layers\n",
        "   - c) CNN layer\n",
        "   - d) Attention layer\n",
        "\n",
        "7. What is masked attention used for?\n",
        "   - a) Feature selection\n",
        "   - b) Prevent future information leak\n",
        "   - c) Reduce computation\n",
        "   - d) Improve accuracy\n",
        "\n",
        "8. What is the key advantage of transformers over RNNs?\n",
        "   - a) Less memory usage\n",
        "   - b) Parallel processing\n",
        "   - c) Simpler architecture\n",
        "   - d) Better accuracy\n",
        "\n",
        "9. What is the purpose of residual connections?\n",
        "   - a) Feature extraction\n",
        "   - b) Gradient flow\n",
        "   - c) Memory efficiency\n",
        "   - d) Regularization\n",
        "\n",
        "10. Which is NOT a component of transformer block?\n",
        "    - a) Multi-head attention\n",
        "    - b) Feed-forward network\n",
        "    - c) Layer normalization\n",
        "    - d) Convolution layer\n",
        "\n",
        "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-d"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}