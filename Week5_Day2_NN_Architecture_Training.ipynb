{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallelamanojkumar90/AIML/blob/main/Week5_Day2_NN_Architecture_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Xch3RLoZvg"
      },
      "source": [
        "# Week 5, Day 2: Neural Network Architecture and Training\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different neural network architectures\n",
        "- Learn optimization techniques\n",
        "- Master regularization methods\n",
        "- Practice building and training networks\n",
        "\n",
        "## Topics Covered\n",
        "1. Network Architectures\n",
        "2. Optimization Methods\n",
        "3. Regularization Techniques\n",
        "4. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brcbNi8EoZvo"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDI3Etz_oZvt"
      },
      "source": [
        "## 1. Network Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ0lmWWVoZvu"
      },
      "source": [
        "def compare_architectures():\n",
        "    # Generate synthetic data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 10)\n",
        "    y = np.sum(X**2, axis=1) + np.random.normal(0, 0.1, 1000)\n",
        "\n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define different architectures\n",
        "    def create_shallow_network():\n",
        "        model = Sequential([\n",
        "            Dense(32, activation='relu', input_shape=(10,)),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def create_deep_network():\n",
        "        model = Sequential([\n",
        "            Dense(16, activation='relu', input_shape=(10,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def create_wide_network():\n",
        "        model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(10,)),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    # Create and compile models\n",
        "    models = {\n",
        "        'Shallow': create_shallow_network(),\n",
        "        'Deep': create_deep_network(),\n",
        "        'Wide': create_wide_network()\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train and evaluate models\n",
        "    histories = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name} Network:\")\n",
        "        history = model.fit(X_train_scaled, y_train,\n",
        "                          validation_split=0.2,\n",
        "                          epochs=50,\n",
        "                          verbose=0)\n",
        "        histories[name] = history.history\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history['loss'], label=f'{name} (train)')\n",
        "        plt.plot(history['val_loss'], '--', label=f'{name} (val)')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Compare final performance\n",
        "    results = []\n",
        "    for name, model in models.items():\n",
        "        test_loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "        results.append({'Architecture': name, 'Test Loss': test_loss})\n",
        "\n",
        "    plt.subplot(122)\n",
        "    results_df = pd.DataFrame(results)\n",
        "    sns.barplot(data=results_df, x='Architecture', y='Test Loss')\n",
        "    plt.title('Test Performance')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_architectures()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3i7Du26oZvv"
      },
      "source": [
        "## 2. Optimization Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QIYkRlooZvw"
      },
      "source": [
        "def compare_optimizers():\n",
        "    # Generate data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 5)\n",
        "    y = np.sum(X**2, axis=1) + np.random.normal(0, 0.1, 1000)\n",
        "\n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define optimizers\n",
        "    optimizers = {\n",
        "        'SGD': SGD(learning_rate=0.01),\n",
        "        'Adam': Adam(learning_rate=0.01),\n",
        "        'RMSprop': RMSprop(learning_rate=0.01)\n",
        "    }\n",
        "\n",
        "    # Create and train models\n",
        "    histories = {}\n",
        "    for name, optimizer in optimizers.items():\n",
        "        model = Sequential([\n",
        "            Dense(32, activation='relu', input_shape=(5,)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "        print(f\"\\nTraining with {name}:\")\n",
        "        history = model.fit(X_train_scaled, y_train,\n",
        "                          validation_split=0.2,\n",
        "                          epochs=50,\n",
        "                          verbose=0)\n",
        "        histories[name] = history.history\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history['loss'], label=f'{name} (train)')\n",
        "        plt.plot(history['val_loss'], '--', label=f'{name} (val)')\n",
        "\n",
        "    plt.title('Optimizer Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "compare_optimizers()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8L6Qo9DoZvx"
      },
      "source": [
        "## 3. Regularization Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-UKSD_roZvx"
      },
      "source": [
        "def compare_regularization():\n",
        "    # Generate data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 20)\n",
        "    y = np.sum(X[:, :5]**2, axis=1) + np.random.normal(0, 0.1, 1000)\n",
        "\n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define models with different regularization\n",
        "    def create_base_model():\n",
        "        return Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(20,)),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    def create_l2_model():\n",
        "        return Sequential([\n",
        "            Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(20,)),\n",
        "            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    def create_dropout_model():\n",
        "        return Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(20,)),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    def create_batchnorm_model():\n",
        "        return Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(20,)),\n",
        "            BatchNormalization(),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    models = {\n",
        "        'Base': create_base_model(),\n",
        "        'L2': create_l2_model(),\n",
        "        'Dropout': create_dropout_model(),\n",
        "        'BatchNorm': create_batchnorm_model()\n",
        "    }\n",
        "\n",
        "    # Train models\n",
        "    histories = {}\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        print(f\"\\nTraining {name} Model:\")\n",
        "        history = model.fit(X_train_scaled, y_train,\n",
        "                          validation_split=0.2,\n",
        "                          epochs=50,\n",
        "                          verbose=0)\n",
        "        histories[name] = history.history\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history['loss'], label=f'{name} (train)')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history['val_loss'], label=name)\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_regularization()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRXZSdxvoZvz"
      },
      "source": [
        "## Practical Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqAk9ut_oZv0"
      },
      "source": [
        "# Exercise 1: Architecture Design\n",
        "\n",
        "def architecture_exercise():\n",
        "    # Generate complex dataset\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "\n",
        "    t = np.random.uniform(0, 2*np.pi, n_samples)\n",
        "    X = np.column_stack([\n",
        "        np.sin(t) + np.random.normal(0, 0.1, n_samples),\n",
        "        np.cos(t) + np.random.normal(0, 0.1, n_samples)\n",
        "    ])\n",
        "    y = np.sin(2*t) + 0.5*np.cos(3*t) + np.random.normal(0, 0.1, n_samples)\n",
        "\n",
        "    # Plot data\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
        "    plt.colorbar(label='Target')\n",
        "    plt.title('Complex Dataset')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Task: Design a neural network architecture\")\n",
        "    print(\"1. Experiment with different depths and widths\")\n",
        "    print(\"2. Try different activation functions\")\n",
        "    print(\"3. Add appropriate regularization\")\n",
        "    print(\"4. Compare performance\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "architecture_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHzti40loZv2"
      },
      "source": [
        "# Exercise 2: Optimization Challenge\n",
        "\n",
        "def optimization_exercise():\n",
        "    # Generate data with multiple local minima\n",
        "    np.random.seed(42)\n",
        "    X = np.random.uniform(-5, 5, (1000, 2))\n",
        "    y = np.sin(X[:, 0]) * np.cos(X[:, 1]) + \\\n",
        "        0.1 * (X[:, 0]**2 + X[:, 1]**2) + \\\n",
        "        np.random.normal(0, 0.1, 1000)\n",
        "\n",
        "    # Plot target function\n",
        "    xx, yy = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n",
        "    zz = np.sin(xx) * np.cos(yy) + 0.1 * (xx**2 + yy**2)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contour(xx, yy, zz, levels=20)\n",
        "    plt.colorbar(label='Target')\n",
        "    plt.title('Complex Target Function')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Task: Optimize neural network training\")\n",
        "    print(\"1. Try different optimizers\")\n",
        "    print(\"2. Tune learning rates\")\n",
        "    print(\"3. Implement learning rate scheduling\")\n",
        "    print(\"4. Compare convergence\")\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "optimization_exercise()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V55NkLycoZv3"
      },
      "source": [
        "## MCQ Quiz\n",
        "\n",
        "1. What is the advantage of deep networks?\n",
        "   - a) Faster training\n",
        "   - b) Less parameters\n",
        "   - c) Hierarchical feature learning\n",
        "   - d) Simpler optimization\n",
        "\n",
        "2. Which optimizer is most commonly used?\n",
        "   - a) SGD\n",
        "   - b) Adam\n",
        "   - c) RMSprop\n",
        "   - d) Adagrad\n",
        "\n",
        "3. What does L2 regularization do?\n",
        "   - a) Adds bias\n",
        "   - b) Penalizes large weights\n",
        "   - c) Speeds up training\n",
        "   - d) Increases capacity\n",
        "\n",
        "4. What is the purpose of dropout?\n",
        "   - a) Feature selection\n",
        "   - b) Prevent overfitting\n",
        "   - c) Speed up training\n",
        "   - d) Improve accuracy\n",
        "\n",
        "5. When should you use batch normalization?\n",
        "   - a) Only in shallow networks\n",
        "   - b) Only in deep networks\n",
        "   - c) With any architecture\n",
        "   - d) Never with dropout\n",
        "\n",
        "6. What is the vanishing gradient problem?\n",
        "   - a) Loss becomes zero\n",
        "   - b) Gradients become too small\n",
        "   - c) Network becomes too large\n",
        "   - d) Learning rate is too high\n",
        "\n",
        "7. Which is NOT a way to prevent overfitting?\n",
        "   - a) Dropout\n",
        "   - b) L2 regularization\n",
        "   - c) Increasing model size\n",
        "   - d) Early stopping\n",
        "\n",
        "8. What does learning rate scheduling do?\n",
        "   - a) Increases model capacity\n",
        "   - b) Adjusts learning rate during training\n",
        "   - c) Selects best architecture\n",
        "   - d) Prevents overfitting\n",
        "\n",
        "9. Which is true about skip connections?\n",
        "   - a) Only used in CNNs\n",
        "   - b) Help gradient flow\n",
        "   - c) Increase parameters\n",
        "   - d) Slow down training\n",
        "\n",
        "10. What is the benefit of wide networks?\n",
        "    - a) Less parameters\n",
        "    - b) Better generalization\n",
        "    - c) More feature capacity\n",
        "    - d) Faster training\n",
        "\n",
        "Answers: 1-c, 2-b, 3-b, 4-b, 5-c, 6-b, 7-c, 8-b, 9-b, 10-c"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}