{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallelamanojkumar90/AIML/blob/main/Week5_Day7_Review_and_FeedBack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwILLgljUUE"
      },
      "source": [
        "# Week 5, Day 7: Review and Feedback Session\n",
        "\n",
        "## Session Overview\n",
        "This session will review the key concepts covered in Week 5 and provide practice exercises to reinforce learning:\n",
        "\n",
        "1. Neural Network Fundamentals\n",
        "2. Convolutional Neural Networks\n",
        "3. Recurrent Neural Networks\n",
        "4. Transformers and Attention\n",
        "5. Deep Learning Applications\n",
        "\n",
        "## Learning Objectives\n",
        "- Reinforce deep learning concepts\n",
        "- Practice model selection\n",
        "- Master implementation skills\n",
        "- Prepare for advanced topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKFO_nJ1jUUL"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Conv2D, MaxPooling2D, LSTM, GRU,\n",
        "    Flatten, Dropout, MultiHeadAttention\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5wjkk2-jUUS"
      },
      "source": [
        "## 1. Neural Network Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w46G_85yjUUV"
      },
      "source": [
        "def neural_network_review():\n",
        "    # Generate synthetic data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 10)\n",
        "    y = np.sum(X**2, axis=1) + np.random.normal(0, 0.1, 1000)\n",
        "\n",
        "    # Create different architectures\n",
        "    def create_shallow_network():\n",
        "        return Sequential([\n",
        "            Dense(32, activation='relu', input_shape=(10,)),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    def create_deep_network():\n",
        "        return Sequential([\n",
        "            Dense(16, activation='relu', input_shape=(10,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    # Train and evaluate models\n",
        "    models = {\n",
        "        'Shallow': create_shallow_network(),\n",
        "        'Deep': create_deep_network()\n",
        "    }\n",
        "\n",
        "    histories = {}\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        histories[name] = model.fit(\n",
        "            X, y,\n",
        "            validation_split=0.2,\n",
        "            epochs=50,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history.history['loss'], label=f'{name} (train)')\n",
        "        plt.plot(history.history['val_loss'], '--', label=f'{name} (val)')\n",
        "\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "neural_network_review()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU9Yq9NljUUW"
      },
      "source": [
        "## 2. CNN Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uai9tMrjUUX"
      },
      "source": [
        "def cnn_review():\n",
        "    # Load MNIST data\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "    X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "    # Create model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Train model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=5,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='test')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='test')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "cnn_review()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d2jRd_-jUUY"
      },
      "source": [
        "## 3. RNN Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Hdehg3jUUZ"
      },
      "source": [
        "def rnn_review():\n",
        "    # Generate sequence data\n",
        "    def generate_sequences(n_samples=1000):\n",
        "        X = np.random.randn(n_samples, 20, 1)\n",
        "        y = np.sum(X[:, -3:, 0], axis=1)\n",
        "        return X, y\n",
        "\n",
        "    # Generate data\n",
        "    X, y = generate_sequences()\n",
        "\n",
        "    # Create different RNN models\n",
        "    def create_lstm_model():\n",
        "        return Sequential([\n",
        "            LSTM(32, input_shape=(20, 1)),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    def create_gru_model():\n",
        "        return Sequential([\n",
        "            GRU(32, input_shape=(20, 1)),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "    # Train and evaluate models\n",
        "    models = {\n",
        "        'LSTM': create_lstm_model(),\n",
        "        'GRU': create_gru_model()\n",
        "    }\n",
        "\n",
        "    histories = {}\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        histories[name] = model.fit(\n",
        "            X, y,\n",
        "            validation_split=0.2,\n",
        "            epochs=20,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history.history['loss'], label=f'{name} (train)')\n",
        "        plt.plot(history.history['val_loss'], '--', label=f'{name} (val)')\n",
        "\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "rnn_review()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvwrKcfvjUUa"
      },
      "source": [
        "## 4. Transformer Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UloRQUcijUUc"
      },
      "source": [
        "def transformer_review():\n",
        "    # Generate sequence data\n",
        "    sequence_length = 20\n",
        "    n_features = 32\n",
        "    n_samples = 1000\n",
        "\n",
        "    X = np.random.randn(n_samples, sequence_length, n_features)\n",
        "    y = np.mean(X, axis=1)\n",
        "\n",
        "    # Create transformer model\n",
        "    inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32)(inputs, inputs)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = Dense(n_features)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Train model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        validation_split=0.2,\n",
        "        epochs=20,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.title('Transformer Model Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "transformer_review()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD-9ltmPjUUe"
      },
      "source": [
        "## Week 5 Review Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHz_nD5jUUf"
      },
      "source": [
        "### Multiple Choice Questions\n",
        "\n",
        "1. Which activation function is most commonly used in hidden layers?\n",
        "   - a) Sigmoid\n",
        "   - b) ReLU\n",
        "   - c) Tanh\n",
        "   - d) Linear\n",
        "\n",
        "2. What is the purpose of convolution in CNNs?\n",
        "   - a) Dimensionality reduction\n",
        "   - b) Feature extraction\n",
        "   - c) Classification\n",
        "   - d) Regularization\n",
        "\n",
        "3. Which RNN variant is best for long sequences?\n",
        "   - a) Simple RNN\n",
        "   - b) LSTM\n",
        "   - c) GRU\n",
        "   - d) Bidirectional RNN\n",
        "\n",
        "4. What is the main innovation of transformers?\n",
        "   - a) Convolution operations\n",
        "   - b) Self-attention\n",
        "   - c) Recurrent connections\n",
        "   - d) Dense layers\n",
        "\n",
        "5. Which is NOT a type of regularization?\n",
        "   - a) Dropout\n",
        "   - b) L2 regularization\n",
        "   - c) Batch normalization\n",
        "   - d) Gradient descent\n",
        "\n",
        "6. What is transfer learning?\n",
        "   - a) Data augmentation\n",
        "   - b) Using pre-trained models\n",
        "   - c) Model compression\n",
        "   - d) Ensemble learning\n",
        "\n",
        "7. Which optimizer is most commonly used?\n",
        "   - a) SGD\n",
        "   - b) Adam\n",
        "   - c) RMSprop\n",
        "   - d) Adagrad\n",
        "\n",
        "8. What is the purpose of pooling layers?\n",
        "   - a) Feature extraction\n",
        "   - b) Dimensionality reduction\n",
        "   - c) Activation\n",
        "   - d) Regularization\n",
        "\n",
        "9. Which is true about batch normalization?\n",
        "   - a) Only used in CNNs\n",
        "   - b) Speeds up training\n",
        "   - c) Increases model size\n",
        "   - d) Reduces accuracy\n",
        "\n",
        "10. What is the vanishing gradient problem?\n",
        "    - a) Loss becomes zero\n",
        "    - b) Gradients become too small\n",
        "    - c) Model becomes too large\n",
        "    - d) Learning rate is too high\n",
        "\n",
        "Answers: 1-b, 2-b, 3-b, 4-b, 5-d, 6-b, 7-b, 8-b, 9-b, 10-b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42y92wvOjUUh"
      },
      "source": [
        "## Week 5 Summary\n",
        "\n",
        "### Key Concepts Covered:\n",
        "1. Neural network architectures and training\n",
        "2. Convolutional neural networks\n",
        "3. Recurrent neural networks\n",
        "4. Transformers and attention mechanisms\n",
        "\n",
        "### Preparation for Advanced Topics:\n",
        "- Review challenging concepts\n",
        "- Practice implementation\n",
        "- Study real-world applications\n",
        "- Explore latest research\n",
        "\n",
        "### Additional Resources:\n",
        "- Deep Learning book: https://www.deeplearningbook.org/\n",
        "- TensorFlow tutorials: https://www.tensorflow.org/tutorials\n",
        "- Research papers: https://paperswithcode.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}